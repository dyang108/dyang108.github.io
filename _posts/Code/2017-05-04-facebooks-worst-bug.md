---
layout: post
title: Facebook's worst bug
category: code 
date: 2017-05-04
---

A recent [New York Times article](https://www.nytimes.com/2017/04/25/magazine/can-facebook-fix-its-own-worst-bug.html) points out the impact that Facebook has had on isolating people into their political corners.

This is probably one of the biggest challenges of AI today: personalization of the News Feed has gone to the next level, and Facebook is suddenly responsible for the reinforcement of political ideologies. We can blame the existence of partisanship today at least partially on artificial intelligence.

Maybe give the user some ability to select how much they are sheltered in their political bubble? Maybe a "hmm... I don't agree, but tell me more" button.

It makes me a bit curious about how Facebook is using data generated from "reacts" to play into their News Feed algorithm. Just because someone "angry reacts" at something doesn't necessarily mean they don't want to see it. How to differentiate? And how can Facebook create bipartisan News Feeds that people actually want to see?
